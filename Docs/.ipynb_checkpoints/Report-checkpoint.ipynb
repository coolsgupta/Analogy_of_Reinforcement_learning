{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING ENGINEER NANODEGREE\n",
    "## CAPSTONE PROJECT \n",
    "## ANALOGY OF REINFORCEMENT LEARNING\n",
    "### Sachin Gupta\n",
    "### May 6th, 2018\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "In this project, we have done a comparative study of conventional Table based Q learning algorithm with Deep Q learning algorithm. The aim was to compare and contrast the performance of various algorithms on similar environment. We have used â€˜OpenAI Gymâ€™ environments like Cartpole and Mountain Car to do a comparative study of Deep Q learning and conventional Q learning algorithm. The results show that where conventional Q learning algorithms couldnâ€™t even achieve satisfactory results, Deep Q algorithms achieved almost perfect score in the same game environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "Reinforcement Learning is an area of machine learning, where an agent learns by interacting with its environment to achieve an objective. In the conventional method the model uses a Q table to store and retrieve the actions for every state based on the rewards earned by the model in the past. This lookup Q table helps the model decide the future course of action. Though this model can be efficient for small state space systems, a system with large number of state-action pairs cannot be handled using this conventional method. Also, this method employs a lookup table for the system which makes it limited to that particular system and for each new system a new model has to be designed which makes it inefficient and makes it distant from human-like learning.\n",
    "<br>\n",
    "<br>\n",
    "In contrast Deep learning employs neural networks which make it closer to human-like learning as a single model can be used to work on different environments without any prior knowledge of the environment. The model learns by itself through the rewards it achieves on taking certain actions. Thus, a single model can be used to work on similar but different environments viz gaming environments that involve maximizing a particular score based on the actions taken on any state. Using Deep Q Learning a single model trained for a particular environment can be made to work on other models too which may have different rules, states and set of action. Thus, the model is not limited to state-action pairs and is flexible to work on different environments too without much modification.<br>\n",
    "<br>\n",
    "For this project we have used two environments designed and developed by â€˜OpenAI Gymâ€™. These are â€˜Cartpole-V1â€™ and â€˜MountainCar-V0â€™ (explained in detail in the environment and dataset section)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "1.\tTo study Reinforcement learning and in particular Q Learning\n",
    "2.\tTo study about conventional Q learning algorithms and to implement the algorithms on OpenAI gym environments\n",
    "3.\tTo study about Deep Q learning algorithms and to implement the algorithm on OpenAI gym environments \n",
    "4.\tTo compare and contrast the performance and results of the two algorithm in the considered environments using the respective metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "<br>\n",
    "The models involved in the project has been compared and contrasted on the basis of four evaluation metrics. There are:\n",
    "1.\t<b>Score</b>: This evaluation metric is based on the goal of the environment and denotes how close the agent got to reaching the goal for each episode independently. This metric is independent of independent actions as the agent may take some wrong actions but if it manages to reach the goal state it has earned the score awarded.<br>\n",
    "2.\t<b>Total Reward</b>: The evaluation metric provided by OpenAI Gym provides the reward earned by the agent for each action that it takes. For comparison we took a sum of all the rewards earned by the agent in an episode and used it as an evaluation metric called the Total Reward. Unlike the score, this is affected by each action taken by the agent and hence even if the agent manages to reach the goal state, it may encounter negative rewards for any wrong action chosen.<br>\n",
    "3.\t<b>Average Score</b>: Since the instantaneous scores can be very random and hence hard to study, we smoothened the curve using the averages. The average score depicts the mean of the scores earned by the agent up to the current episode as shown in the results section of this report.<br>\n",
    "4.\t<b>Average Reward</b>: Similar to score, the instantaneous rewards can also be very random and hence hard to study, we smoothened the curve using the averages. The average reward depicts the mean of the total rewards earned by the agent up to the current episode as shown in the results section of this report.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms\n",
    "## Reinforcement Learning\n",
    "\n",
    "<img src=\"random images/figure 1.png\" height = 50% width = 50% title = \"Figure 1: Reinforcement Learning\">\n",
    "<center>Figure 1: Reinforcement Learning</center>\n",
    "<br>\n",
    "<b>Reinforcement Learning</b> is a type of machine learning that allows you to create AI agents that learn from the environment by interacting with it. Reinforcement learning provides the capacity for us not only to teach an artificial agent how to act, but to allow it to learn through itâ€™s own interactions with an environment.<br>\n",
    "<br>\n",
    "One of the most important aspects of reinforcement learning is that it takes into account that taking an action does not necessarily only affect the immediate reward, but it may have an impact in the future rewards as well. This way, taking what seems to be the best action now may not be the best decision in the long term. This concept is known as <b>delayed reward</b>.<br>\n",
    "<br>\n",
    "Another of the key aspects of reinforcement learning is that in order to maximize the reward we must follow what we have learned is best, but in order to learn that we have to also explore randomly sometimes. This is called the <b>exploration-exploitation dilemma</b>.<br>\n",
    "<br>\n",
    "## Q Learning Algorithm\n",
    "Q Learning is a popular, <b>off-policy</b> learning algorithm that utilizes the <b>Q function</b>. It is based on the simple premise that the best policy is the one that selects the action with the highest total future reward.<br>\n",
    "<br>\n",
    "In a reinforcement learning model, an agent takes actions in an environment with the goal of maximizing a cumulative reward. The basic reinforcement learning model consists of: a set of environment states S; a set of actions A; rules of transitioning between states; rules that determine the scalar immediate reward of a transition; and rules that describe what the agent observes.<br>\n",
    "<br>\n",
    "Q-Learning is a <b>model-free</b> form of Reinforcement Learning. If S is a set of states, A is a set of actions, ğœ¸ is the discount factor, is the step size. Then we can understand Q-Learning by this. <br>\n",
    "<br>\n",
    "Algorithm: <br>\n",
    "ğ¼ğ‘›ğ‘–ğ‘¡ğ‘–ğ‘ğ‘™ğ‘–ğ‘§ğ‘’ ğ‘¸(ğ’”, ğ’‚) ğ‘ğ‘Ÿğ‘ğ‘–ğ‘¡ğ‘Ÿğ‘ğ‘Ÿğ‘–ğ‘™ğ‘¦<br> \n",
    "ğ‘…ğ‘’ğ‘ğ‘’ğ‘ğ‘¡ (ğ‘“ğ‘œğ‘Ÿ ğ‘’ğ‘ğ‘â„ ğ‘’ğ‘ğ‘–ğ‘ ğ‘œğ‘‘ğ‘’): <br>\n",
    "    ğ¼ğ‘›ğ‘–ğ‘¡ğ‘–ğ‘ğ‘™ğ‘–ğ‘§ğ‘’ ğ‘º <br>\n",
    "    ğ‘…ğ‘’ğ‘ğ‘’ğ‘ğ‘¡ (ğ‘“ğ‘œğ‘Ÿ ğ‘’ğ‘ğ‘â„ ğ‘ ğ‘¡ğ‘’ğ‘ ğ‘œğ‘“ ğ‘’ğ‘ğ‘–ğ‘ ğ‘œğ‘‘ğ‘’): <br>\n",
    "    ğ¶â„ğ‘œğ‘œğ‘ ğ‘’ ğ’‚ ğ‘“ğ‘Ÿğ‘œğ‘š ğ’” ğ‘¢ğ‘ ğ‘–ğ‘›ğ‘” ğ‘ğ‘œğ‘™ğ‘–ğ‘ğ‘¦ ğ‘‘ğ‘’ğ‘Ÿğ‘–ğ‘£ğ‘’ğ‘‘ ğ‘“ğ‘Ÿğ‘œğ‘š ğ‘¸ (ğ‘’. ğ‘”. âˆˆ âˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)<br>\n",
    "    ğ‘‡ğ‘ğ‘˜ğ‘’ ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ’‚, ğ‘œğ‘ğ‘ ğ‘’ğ‘Ÿğ‘£ğ‘’ ğ’“, ğ’”â€² ğ‘¸â€²(ğ’”â€², ğ’‚â€²) < âˆ’ âˆ’ ğ‘¸(ğ’”, ğ’‚) + ğœ¶[ğ’“ + ğœ¸. ğ’ğ’‚ğ’™ ğ‘¸(ğ’” â€² , ğ’‚ â€² ) âˆ’ ğ‘¸(ğ’”, ğ’‚)] <br>\n",
    "    ğ’” < âˆ’ âˆ’ ğ’” â€² <br>\n",
    "    ğ‘¢ğ‘›ğ‘¡ğ‘–ğ‘™ ğ’” ğ‘–ğ‘  ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘–ğ‘›ğ‘l<br>\n",
    "\n",
    "In Q-learning, the value of the Q function for each state is updated iteratively based on the new rewards it receives. At its most basic level, the algorithm looks at the difference between (a) its current estimate of total future reward and (b) the estimate generated from its most recent experience. After it calculates this difference (a - b), it adjusts its current estimate up or down a bit based on the number. Q-learning uses a couple of parameters to allow us to tweak how the process works. The first is the learning rate, represented by the Greek letter Î± (alpha). This is a number between 0 and 1 that determines the extent to which newly learned information will impact the existing estimates. A low Î± means that the agent will put less stock into the new information it learns, and a high Î± means that new information will more quickly override older information. The second parameter is the discount rate, g. The higher the discount rate, the less important future rewards are valued.<br>\n",
    "<br>\n",
    "Below is the annotated equation for Q-learning:\n",
    "<img src=\"random images/figure 2.png\" height = 80% width = 80% title = \"Figure 2: Annotated equation for Q Learning\">\n",
    "<center>Figure 2: Annotated equation for Q Learning</center>\n",
    "<br>\n",
    "<img src=\"random images/figure 3.png\" height = 80% width = 80% title = \"Figure 3: Simplified Annotated equation for Q Learning\">\n",
    "<center>Figure 3: Simplified Annotated equation for Q Learning</center>\n",
    "<br>\n",
    "## Deep Q Learning\n",
    "<img src=\"random images/figure 4.png\" height = 80% width = 80% title = \"Figure 4: Deep Q Learning Algorithm\">\n",
    "<center>Figure 4: Deep Q Learning Algorithm</center>\n",
    "<br>\n",
    "Algorithm:<br>\n",
    "initialize state, epsilon, alpha, epsilon<br>\n",
    "while training:  <br>\n",
    "  observe state<br>\n",
    "  feed forward state through NN to get q_matrix<br>\n",
    "  if (epsilon > random_float_between_0_and_1):<br>\n",
    "    select random action<br>\n",
    "  else:<br>\n",
    "    action_idx = index of the element of q_matrix with largest value<br>\n",
    "<br>\n",
    "apply action at action<br>\n",
    "  observe next_state<br>\n",
    "  observe reward<br>\n",
    "  if next_state is terminal:<br>\n",
    "    q_update = reward<br>\n",
    "  else:<br>\n",
    "    feed forward next_state through NN to get q_matrix_next<br>\n",
    "    observe the largest q value (q_max) in q_matrix_next<br>\n",
    "    q_update = reward + gamma*q_max<br>\n",
    "  <br>\n",
    "  make copy of q_matrix (q_target)<br>\n",
    "  q_target[action_idx] = q_update<br>\n",
    "  loss = MSE(q_matrix, q_update)<br>\n",
    "  optimize NN using loss function<br>\n",
    "  state = next_state<br>\n",
    "  epsilon -= decay_rate<br>\n",
    "<br>\n",
    "Normally in games, the reward directly relates to the score of the game. Imagine a situation where the pole from CartPole game is tilted to the right. The expected future reward of pushing right button will then be higher than that of pushing the left button since it could yield higher score of the game as the pole survives longer.<br>\n",
    "<br>\n",
    "In order to logically represent this intuition and train it, we need to express this as a formula that we can optimize on. The loss is just a value that indicates how far our prediction is from the actual target. For example, the prediction of the model could indicate that it sees more value in pushing the right button when in fact it can gain more reward by pushing the left button. We want to decrease this gap between the prediction and the target (loss). We will define our loss function as follows:<br>\n",
    "<br>\n",
    "<img src=\"random images/figure 5.png\" height = 50% width = 50% title = \"Figure 5: Mathematical representation of loss function\">\n",
    "<center>Figure 5: Mathematical representation of loss function</center>\n",
    "<br>\n",
    "\n",
    "We first carry out an action a and observe the reward r and resulting new state s. Based on the result, we calculate the maximum target Q and then discount it so that the future reward is worth less than immediate reward (It is a same concept as interest rate for money. Immediate payment always worth more for same amount of money). Lastly, we add the current reward to the discounted future reward to get the target value. Subtracting our current prediction from the target gives the loss. Squaring this value allows us to punish the large loss value more and treat the negative values same as the positive values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Environment\n",
    "The project being based on the reinforcement learning methods involves use of no pre-designed database but only the experience it gains by encountering various states through the training process. (The environment specific data is expressed and explained further in the report in the Cartpole-V1 and MountainCar-V0 sub headings.)\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "The experience described above is stored in a deque and is randomly sampled to create batches of size 32 which are then fed to the neural network for training after the end of each training trial.\n",
    "The general representation of the experience in the memory is as follows:<br>\n",
    "<b>(state, action, reward, next state, done flag)</b>\n",
    "1.\tState: This denote the current state that the agent is in (environment specific state space is explained further in the report).\n",
    "2.\tAction: This denoted the action taken by the agent in the current state.\n",
    "3.\tReward: this denoted the reward earned by the agent for the current state (calculated as per the loss function for DQN and Q function for table-based q learning agent).\n",
    "4.\tNext State: This denotes the next state that the environment reaches after taking the action in the current state.\n",
    "5.\tDone flag:  This denotes the status of the game. If â€˜Trueâ€™ it marks the end of the episode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole-V1\n",
    "\n",
    "<img src=\"random images/figure 6.png\" height = 500 width = 500 title = \"Figure 6: GUI for Cartpole Environment\">\n",
    "<center>Figure 6: GUI for Cartpole Environment</center>\n",
    "<br>\n",
    "CartPole is one of the simplest environments in OpenAI gym (a game simulator). The goal of CartPole is to balance a pole connected with one joint on top of a moving cart. Instead of pixel information, there are 4 kinds of information given by the state, such as angle of the pole and position of the cart. An agent can move the cart by performing a series of actions of 0 or 1 to the cart, pushing it left or right.<br>\n",
    "<br>\n",
    "### How the Agent Decides to Act\n",
    "Our agent will randomly select its action at first by a certain percentage, called â€˜exploration rateâ€™ or â€˜epsilonâ€™. This is because at first, it is better for the agent to try all kinds of things before it starts to see the patterns. When it is not deciding the action randomly, the agent will predict the reward value based on the current state and pick the action that will give the highest reward.<br>\n",
    "<br>\n",
    "In the beginning, the agent explores by acting randomly.<br>\n",
    "It goes through multiple phases of learning.\n",
    "1.\tThe cart masters balancing the pole.\n",
    "2.\tBut goes out of bounds, ending the game.\n",
    "3.\tIt tries to move away from the bounds when it is too close to them but drops the pole.\n",
    "4.\tThe cart masters balancing and controlling the pole.\n",
    "\n",
    "After several hundreds of episodes, it starts to learn how to maximize the score.\n",
    "The final result is the birth of a skillful CartPole game player!<br>\n",
    "<br>\n",
    "\n",
    "### Environment Description\n",
    "<center>TABLE 1: State Space Representation</center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Num</th>\n",
    "        <th>Observation</th>\n",
    "        <th>Min</th>\n",
    "        <th>Max</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>Cart Position</td>\n",
    "        <td>-2.4</td>\n",
    "        <td>2.4</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>Cart Velocity</td>\n",
    "        <td>-Inf</td>\n",
    "        <td>Inf</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2</td>\n",
    "        <td>Pole Angle</td>\n",
    "        <td>~ -41.8Â°</td>\n",
    "        <td>~41.8Â°</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>3</td>\n",
    "        <td>Pole Velocity At Tip</td>\n",
    "        <td>-Inf</td>\n",
    "        <td>Inf</td>\n",
    "    </tr>        \n",
    "</table>\n",
    "<br>\n",
    "<br>\n",
    "<center>TABLE 2: List of Valid Actions</center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Num</th>\n",
    "        <th>Action</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>Push cart to the left</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>Push cart to the right</td>\n",
    "    </tr>        \n",
    "</table>\n",
    "<br>\n",
    "Note: The amount the velocity is reduced or increased is not fixed as it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it.\n",
    "### Evaluation Criteria\n",
    "1.\tScore (0 to 499) : The number of frames for which the agent manages to balance the pole.\n",
    "\n",
    "2.\tReward : Reward is 1 for every step taken, including the termination step.\n",
    "\n",
    "3.\tAverage Score: The mean of the scores till the respective episode.\n",
    "\n",
    "4.\tAverage Reward: The reward of the scores till the respective episode.\n",
    "\n",
    "\n",
    "### Starting State\n",
    "All observations are assigned a uniform random value between Â±0.05\n",
    "### Episode Termination\n",
    "1.\tPole Angle is more than Â±12Â°\n",
    "2.\tCart Position is more than Â±2.4 (center of the cart reaches the edge of the display)\n",
    "3.\tEpisode length reaches 500 frames.\n",
    "\n",
    "### Goal\n",
    "To balance the pole on the cart for the given number of frames (here, 500 frames).\n",
    "## MountainCar-V0\n",
    "<img src=\"random images/figure 7.png\" height = 500 width = 500 title = \"Figure 7: GUI for Mountain Car environment\">\n",
    "<center>Figure 7: GUI for Mountain Car environment</center>\n",
    "<br>\n",
    "Mountain Car is a problem in which an underpowered car must drive up a steep hill. A car is on a one-dimensional track, positioned between two \"mountains\". The goal is to drive up the mountain on the right; however, the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum. Since gravity is stronger than the car's engine, even at full throttle, the car cannot simply accelerate up the steep slope. The car is situated in a valley and must learn to leverage potential energy by driving up the opposite hill before the car is able to make it to the goal at the top of the rightmost hill.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Mountain Car game Environment\n",
    "\n",
    "<center>TABLE 3: State Space Representation</center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Num</th>\n",
    "        <th>Observation</th>\n",
    "        <th>Min</th>\n",
    "        <th>Max</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>Position</td>\n",
    "        <td>-1.2</td>\n",
    "        <td>0.6/td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>Velocity</td>\n",
    "        <td>-0.07</td>\n",
    "        <td>0.07</td>\n",
    "    </tr>\n",
    "</table>\n",
    "<br>\n",
    "<br>\n",
    "<center>TABLE 4: List of Valid Actions</center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Num</th>\n",
    "        <th>Action</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>Push left</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>No Push</td>\n",
    "    </tr>       \n",
    "    <tr>\n",
    "        <td>2</td>\n",
    "        <td>Push right</td>\n",
    "    </tr>       \n",
    "</table>\n",
    "<br>\n",
    "### EVALUATION CRITERIA\n",
    "1.\tScore: (-1.2 to 0.6) : ): The highest coordinate that the agent manages to reach (Goal 0.5).\n",
    "2.\tReward : -1 for each time step, until the goal position of 0.5 is reached. \n",
    "3.\tAverage Score : The mean of the scores till the respective episode.\n",
    "4.\tAverage Reward : The mean of the rewards till the respective episode.\n",
    "\n",
    "### Starting State\n",
    "Random position from -0.6 to -0.4 with no velocity.\n",
    "\n",
    "### EPISODE TERMINATION\n",
    "The episode ends when you reach 0.5 position, or if 200 iterations are reached.\n",
    "\n",
    "### Goal\n",
    "To reach the position 0.5 in the given number of frames (here, 200 frames max). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Model\n",
    "Since the main objective of this project is to improve the performance of the Table based Q-learning model. It involves a comparative study of Random Exploration model, The Table Based Q learning Model and The Deep Q Learning Model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "The entire code for the project has been saved in the repository at:\n",
    "https://github.com/coolsgupta/Udacity_Capstone.git.\n",
    "\n",
    "<img src=\"random images/figure 9.png\" height = 50% width = 50% title = \"Figure 8: The Random Exploration agent\">\n",
    "<center>Figure 8: The Random Exploration agent</center>\n",
    "<br>\n",
    "\n",
    "<img src=\"random images/figure 10.png\" height = 120% width = 120% title = \"Figure 9: The Q Learning agent\">\n",
    "<center>Figure 9: The Q Learning agent</center>\n",
    "<br>\n",
    " \n",
    "<img src=\"random images/figure 8.png\" height = 60% width = 60% title = \"Figure 10: The Deep Q Learning Model\">\n",
    "<center>Figure 10: The Deep Q Learning Model</center>\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and Results\n",
    "## Learning Curves\n",
    "### Table Based Q Learning Agent\n",
    "\n",
    "<img src=\"Cartpole Results/outputs/QL exploration factor decay.png\" height = 60% width = 60%>\n",
    "<center>Figure 11: Decay of Exploration Factor (Cartpole and Mountain Car)</center>\n",
    "<br>\n",
    "\n",
    "The Figure 11 depicts the decay of exploration factor with the increase in the number of trials demanding the agent to take more actions from experience than random exploration. Also, the training terminates when the exploration factor drops to 0.01.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td>\n",
    "        <img src=\"Cartpole Results/outputs/QL_score.png\">\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"Mountain Car Results/outputs/QL_score.png\">\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>\n",
    "        <center>(a): Cartpole</center>\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>(b): Mountain Car</center>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "<center>Figure 12: Score vs Episodes(Q-learning)</center>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td>\n",
    "        <img src=\"Cartpole Results/outputs/QL_total_reward.png\">\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"Mountain Car Results/outputs/QL_total_reward.png\">\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>\n",
    "        <center>(a): Cartpole</center>\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>(b): Mountain Car</center>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "<center>Figure 13: Reward vs Episodes(Q-learning)</center>\n",
    "\n",
    "The Figure 12  and Figure 13 show the performance of the Q learning agent when training for the cartpole environment (a) and the Mountain Car environment (b), as observed, the agent achieves a maximum score of 128 and a maximum total reward of 118 in trial number 3161 for Cartpole and a maximum score of -0.03 in trial number 2119 and a constant total reward of -209 (showing that it never reached the goal state) for Mountain Car, making it apparent that the overall performance  remains low and random.\n",
    " \n",
    "<table>\n",
    "<tr>\n",
    "    <td>\n",
    "        <img src=\"Cartpole Results/outputs/QL_avg_score.png\">\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"Mountain Car Results/outputs/QL_avg_score.png\">\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>\n",
    "        <center>(a): Cartpole</center>\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>(b): Mountain Car</center>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "<center>Figure 14: Average Score vs Episodes(Q-learning)</center>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td>\n",
    "        <img src=\"Cartpole Results/outputs/QL_avg_rev.png\">\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"Mountain Car Results/outputs/QL_avg_rev.png\">\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>\n",
    "        <center>(a): Cartpole</center>\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>(b): Mountain Car</center>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "<center>Figure 15: Average reward vs Episodes(Q-learning)</center>\n",
    "\n",
    "As shown in figure 14 and figure 15 the average score and average rewards per episode of the Q learning agent remain very low as 21.33 and 11.3 respectively for cartpole and -0.39 and -208.95 for Mountain car. Also, they dropped as low as 11.5 and 4.5 and reaching just 21.56 and 11.57 as the maximum values for Cartpole. For Mountain Car the average score dropped to -0.39 while the average score remained close to -209 showing that the agent never reached the goal state. The cause for the same is shown in the figure 16, i.e. the growth of the Q table. \n",
    "The Q learning agent when encounters a new state, creates an entry in the Q table and takes a random action for the instance and updates the Q values of the actions for that state accordingly so that it can refer to the same in order to possibly take a better action when it encounters the same state again. This leads to the problem known as the growth rate of the Q table. The same can be observed in figure 16 as it shows that by end of trial number 4605 the table already consisted of 107436 states for Cartpole and 917784 for Mountain Car and yet it can be observed from the slope that it would certainly increase further if subjected to more number of trials.\n",
    " \n",
    "<table>\n",
    "<tr>\n",
    "    <td>\n",
    "        <img src=\"Cartpole Results/outputs/QL Q Table length.png\">\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"Mountain Car Results/outputs/QL Q Table length.png\">\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>\n",
    "        <center>(a): Cartpole</center>\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>(b): Mountain Car</center>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "<center>Figure 16: Growth of the Q table(Q-learning)</center>\n",
    "\n",
    "\n",
    "### Deep-Q-Learning Agent\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td>\n",
    "        <img src=\"Cartpole Results/outputs/dql_decay.png\">\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"Mountain Car Results/outputs/dql_decay.png\">\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>\n",
    "        <center>(a): Cartpole</center>\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>(b): Mountain Car</center>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "<center>Figure 17: Epsilon Decay rate(DQN)</center>\n",
    "<table>\n",
    "<tr>\n",
    "    <td>\n",
    "        <img src=\"Cartpole Results/outputs/dql_score.png\">\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"Mountain Car Results/outputs/dql_score.png\">\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>\n",
    "        <center>(a): Cartpole</center>\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>(b): Mountain Car</center>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "<center>Figure 18: Score vs Episode (DQN)</center>\n",
    "\n",
    " \n",
    "<table>\n",
    "<tr>\n",
    "    <td>\n",
    "        <img src=\"Cartpole Results/outputs/dql_total_reward.png\">\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"Mountain Car Results/outputs/dql_total_reward.png\">\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>\n",
    "        <center>(a): Cartpole</center>\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>(b): Mountain Car</center>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "<center>Figure 19: Reward vs Episode (DQN)</center>\n",
    "\n",
    "\n",
    "Figure 18 and Figure 19 show the performance of the Deep Q learning agent during training. The agent shows apparent growth in the score and total reward and manages to attain maximum score of 499 (perfect score for balancing the pole for all 500 frames) and maximum reward of 489 for Cartpole and a maximum score of 0.54 (goal state) and a maximum reward of -92.0 in the later trials. The agent manages to achieve the same in just 500 trials for cartpole and 100 trials for Mountain Car attaining maximum scores in nearly 300 trials. \n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td>\n",
    "        <img src=\"Cartpole Results/outputs/dql_avg_score.png\">\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"Mountain Car Results/outputs/dql_avg_score.png\">\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>\n",
    "        <center>(a): Cartpole</center>\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>(b): Mountain Car</center>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "<center>Figure 20: Average Score vs Episode (DQN)</center>\n",
    " \n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td>\n",
    "        <img src=\"Cartpole Results/outputs/dql_avg_rewards.png\">\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"Mountain Car Results/outputs/dql_avg_rewards.png\">\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>\n",
    "        <center>(a): Cartpole</center>\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>(b): Mountain Car</center>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "<center>Figure 21: Average Reward vs Episode (DQN)</center>\n",
    "\n",
    "The Figure 20 and Figure 21 show the growth in the average score and average reward depicting the success of the agent to learn the patterns and policies of the environment. \n",
    "The best part of the process being the non-requirement of the Q table as the agent updates the weights of the underlying neural network as per the experienced gained which result in better prediction of the Q values of the actions associated with all the states that the agent may encounter, unlike Q learning where only the value of the visited state was being updated leaving the other states unaffected. Hence enabling the agent to take better decision for both known and unprecedented states.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Videos of Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table>\n",
       "<center> Deep Q Learning agent Learning Videos</center>\n",
       "<tr>\n",
       "    <td>\n",
       "        <div align=\"middle\">\n",
       "        <video width = 90% height = 90% controls>\n",
       "            <source type =\"video/mp4\" src=\"Cartpole Results/Snaps/Cartpole Q_learning_agent episode 1-100.mp4\"/>\n",
       "        </video>\n",
       "        </div>\n",
       "    </td>\n",
       "    <td>\n",
       "        <div align=\"middle\">\n",
       "        <video width = 90% height = 90% controls>\n",
       "            <source type =\"video/mp4\" src=\"Cartpole Results/Snaps/Cartpole DQN_model_1 episode 1-100.mp4\"/>\n",
       "        </video>\n",
       "        </div>\n",
       "    </td>\n",
       "    <td>\n",
       "        <div align=\"middle\">\n",
       "        <video width = 90% height = 90% controls>\n",
       "            <source type =\"video/mp4\" src=\"Mountain Car Results/snaps/MountainCar Q_learning_agent episode 1-10.mp4\"/>\n",
       "        </video>\n",
       "        </div>\n",
       "    </td>\n",
       "    <td>\n",
       "        <div align=\"middle\">\n",
       "        <video width = 90% height = 90% controls>\n",
       "            <source type =\"video/mp4\" src=\"Mountain Car Results/snaps/MountainCar DQN_model_2 episode 1-10.mp4\"/>\n",
       "        </video>\n",
       "        </div>\n",
       "    </td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>\n",
       "        <center>Cartpole episode 1-100 (Q learning)</center>\n",
       "    </td>\n",
       "    <td>\n",
       "        <center>Cartpole episode 1-100 (DQN)</center>\n",
       "    </td>\n",
       "    <td>\n",
       "        <center>Mountain Car episode 1-10 (Q Learning)</center>\n",
       "    </td>\n",
       "    \n",
       "    <td>\n",
       "        <center>Mountain Car episode 1-10 (DQN)</center>\n",
       "    </td>\n",
       "</tr>\n",
       "<tr>\n",
       "    <td>\n",
       "        <div align=\"middle\">\n",
       "        <video width = 90% height = 90% controls>\n",
       "            <source type =\"video/mp4\" src=\"Cartpole Results/Snaps/Cartpole Q_learning_agent episode 2000-2100.mp4\"/>\n",
       "        </video>\n",
       "        </div>\n",
       "    </td>\n",
       "    <td>\n",
       "        <div align=\"middle\">\n",
       "        <video width = 90% height = 90% controls>\n",
       "            <source type =\"video/mp4\" src=\"Cartpole Results/Snaps/Cartpole DQN_model_1 episode 200-250.mp4\"/>\n",
       "        </video>\n",
       "        </div>\n",
       "    </td>\n",
       "    <td>\n",
       "        <div align=\"middle\">\n",
       "        <video width = 90% height = 90% controls>\n",
       "            <source type =\"video/mp4\" src=\"Mountain Car Results/snaps/MountainCar Q_learning_agent episode 2000-2010.mp4\"/>\n",
       "        </video>\n",
       "        </div>\n",
       "    </td>\n",
       "    <td>\n",
       "        <div align=\"middle\">\n",
       "        <video width = 90% height = 90% controls>\n",
       "            <source type =\"video/mp4\" src=\"Mountain Car Results/snaps/MountainCar DQN_model_2 episode 200-210.mp4\"/>\n",
       "        </video>\n",
       "        </div>\n",
       "    </td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>\n",
       "        <center>Cartpole episode 2000-2100 (Q learning)</center>\n",
       "    </td>\n",
       "    <td>\n",
       "        <center>Cartpole episode 200-250 (DQN)</center>\n",
       "    </td>\n",
       "    <td>\n",
       "        <center>Mountain Car episode 2000-2010 (Q Learning)</center>\n",
       "    </td>\n",
       "    \n",
       "    <td>\n",
       "        <center>Mountain Car episode 200-210 (DQN)</center>\n",
       "    </td>\n",
       "</tr>\n",
       "<tr>\n",
       "    <td>\n",
       "        <div align=\"middle\">\n",
       "        <video width = 90% height = 90% controls>\n",
       "            <source type =\"video/mp4\" src=\"Cartpole Results/Snaps/Cartpole Q_learning_agent episode 4500-4600.mp4\"/>\n",
       "        </video>\n",
       "        </div>\n",
       "    </td>\n",
       "    <td>\n",
       "        <div align=\"middle\">\n",
       "        <video width = 90% height = 90% controls>\n",
       "            <source type =\"video/mp4\" src=\"Cartpole Results/Snaps/Cartpole DQN_model_1 episode 450-470.mp4\"/>\n",
       "        </video>\n",
       "        </div>\n",
       "    </td>\n",
       "    <td>\n",
       "        <div align=\"middle\">\n",
       "        <video width = 90% height = 90% controls>\n",
       "            <source type =\"video/mp4\" src=\"Mountain Car Results/snaps/MountainCar Q_learning_agent episode 4500-4510.mp4\"/>\n",
       "        </video>\n",
       "        </div>\n",
       "    </td>\n",
       "    <td>\n",
       "        <div align=\"middle\">\n",
       "        <video width = 90% height = 90% controls>\n",
       "            <source type =\"video/mp4\" src=\"Mountain Car Results/snaps/MountainCar DQN_model_2 episode 450-460.mp4\"/>\n",
       "        </video>\n",
       "        </div>\n",
       "    </td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>\n",
       "        <center>Cartpole episode 4500-4600 (Q learning)</center>\n",
       "    </td>\n",
       "    <td>\n",
       "        <center>Cartpole episode 450-470 (DQN)</center>\n",
       "    </td>\n",
       "    <td>\n",
       "        <center>Mountain Car episode 4500-4510 (Q Learning)</center>\n",
       "    </td>\n",
       "    \n",
       "    <td>\n",
       "        <center>Mountain Car episode 450-460 (DQN)</center>\n",
       "    </td>\n",
       "</tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<table>\n",
    "<center> Deep Q Learning agent Learning Videos</center>\n",
    "<tr>\n",
    "    <td>\n",
    "        <div align=\"middle\">\n",
    "        <video width = 90% height = 90% controls>\n",
    "            <source type =\"video/mp4\" src=\"Cartpole Results/Snaps/Cartpole Q_learning_agent episode 1-100.mp4\"/>\n",
    "        </video>\n",
    "        </div>\n",
    "    </td>\n",
    "    <td>\n",
    "        <div align=\"middle\">\n",
    "        <video width = 90% height = 90% controls>\n",
    "            <source type =\"video/mp4\" src=\"Cartpole Results/Snaps/Cartpole DQN_model_1 episode 1-100.mp4\"/>\n",
    "        </video>\n",
    "        </div>\n",
    "    </td>\n",
    "    <td>\n",
    "        <div align=\"middle\">\n",
    "        <video width = 90% height = 90% controls>\n",
    "            <source type =\"video/mp4\" src=\"Mountain Car Results/snaps/MountainCar Q_learning_agent episode 1-10.mp4\"/>\n",
    "        </video>\n",
    "        </div>\n",
    "    </td>\n",
    "    <td>\n",
    "        <div align=\"middle\">\n",
    "        <video width = 90% height = 90% controls>\n",
    "            <source type =\"video/mp4\" src=\"Mountain Car Results/snaps/MountainCar DQN_model_2 episode 1-10.mp4\"/>\n",
    "        </video>\n",
    "        </div>\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "     <td>\n",
    "        <center>Cartpole episode 1-100 (Q learning)</center>\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>Cartpole episode 1-100 (DQN)</center>\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>Mountain Car episode 1-10 (Q Learning)</center>\n",
    "    </td>\n",
    "    \n",
    "    <td>\n",
    "        <center>Mountain Car episode 1-10 (DQN)</center>\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>\n",
    "        <div align=\"middle\">\n",
    "        <video width = 90% height = 90% controls>\n",
    "            <source type =\"video/mp4\" src=\"Cartpole Results/Snaps/Cartpole Q_learning_agent episode 2000-2100.mp4\"/>\n",
    "        </video>\n",
    "        </div>\n",
    "    </td>\n",
    "    <td>\n",
    "        <div align=\"middle\">\n",
    "        <video width = 90% height = 90% controls>\n",
    "            <source type =\"video/mp4\" src=\"Cartpole Results/Snaps/Cartpole DQN_model_1 episode 200-250.mp4\"/>\n",
    "        </video>\n",
    "        </div>\n",
    "    </td>\n",
    "    <td>\n",
    "        <div align=\"middle\">\n",
    "        <video width = 90% height = 90% controls>\n",
    "            <source type =\"video/mp4\" src=\"Mountain Car Results/snaps/MountainCar Q_learning_agent episode 2000-2010.mp4\"/>\n",
    "        </video>\n",
    "        </div>\n",
    "    </td>\n",
    "    <td>\n",
    "        <div align=\"middle\">\n",
    "        <video width = 90% height = 90% controls>\n",
    "            <source type =\"video/mp4\" src=\"Mountain Car Results/snaps/MountainCar DQN_model_2 episode 200-210.mp4\"/>\n",
    "        </video>\n",
    "        </div>\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "     <td>\n",
    "        <center>Cartpole episode 2000-2100 (Q learning)</center>\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>Cartpole episode 200-250 (DQN)</center>\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>Mountain Car episode 2000-2010 (Q Learning)</center>\n",
    "    </td>\n",
    "    \n",
    "    <td>\n",
    "        <center>Mountain Car episode 200-210 (DQN)</center>\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>\n",
    "        <div align=\"middle\">\n",
    "        <video width = 90% height = 90% controls>\n",
    "            <source type =\"video/mp4\" src=\"Cartpole Results/Snaps/Cartpole Q_learning_agent episode 4500-4600.mp4\"/>\n",
    "        </video>\n",
    "        </div>\n",
    "    </td>\n",
    "    <td>\n",
    "        <div align=\"middle\">\n",
    "        <video width = 90% height = 90% controls>\n",
    "            <source type =\"video/mp4\" src=\"Cartpole Results/Snaps/Cartpole DQN_model_1 episode 450-470.mp4\"/>\n",
    "        </video>\n",
    "        </div>\n",
    "    </td>\n",
    "    <td>\n",
    "        <div align=\"middle\">\n",
    "        <video width = 90% height = 90% controls>\n",
    "            <source type =\"video/mp4\" src=\"Mountain Car Results/snaps/MountainCar Q_learning_agent episode 4500-4510.mp4\"/>\n",
    "        </video>\n",
    "        </div>\n",
    "    </td>\n",
    "    <td>\n",
    "        <div align=\"middle\">\n",
    "        <video width = 90% height = 90% controls>\n",
    "            <source type =\"video/mp4\" src=\"Mountain Car Results/snaps/MountainCar DQN_model_2 episode 450-460.mp4\"/>\n",
    "        </video>\n",
    "        </div>\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "     <td>\n",
    "        <center>Cartpole episode 4500-4600 (Q learning)</center>\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>Cartpole episode 450-470 (DQN)</center>\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>Mountain Car episode 4500-4510 (Q Learning)</center>\n",
    "    </td>\n",
    "    \n",
    "    <td>\n",
    "        <center>Mountain Car episode 450-460 (DQN)</center>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Curves (Q-Learning Agent vs DQN Agent)\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td>\n",
    "        <img src=\"Cartpole Results/outputs/comparison of QL and DQL scores.png \">\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"Mountain Car Results/outputs/comparison of QL and DQL scores.png\">\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>\n",
    "        <center>(a): Cartpole</center>\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>(b): Mountain Car</center>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "<center>Figure 22: Score vs Episode (Q-learning vs DQN)</center>\n",
    " \n",
    "<table>\n",
    "<tr>\n",
    "    <td>\n",
    "        <img src=\"Cartpole Results/outputs/comparison of QL and DQL total rewards.png\">\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"Mountain Car Results/outputs/comparison of QL and DQL total rewards.png\">\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>\n",
    "        <center>(a): Cartpole</center>\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>(b): Mountain Car</center>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "<center>Figure 23: Reward vs Episode (Q-learning vs DQN)</center>\n",
    "\n",
    " \n",
    "<table>\n",
    "<tr>\n",
    "    <td>\n",
    "        <img src=\"Cartpole Results/outputs/comparison of QL and DQL average scores.png\">\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"Mountain Car Results/outputs/comparison of QL and DQL average scores.png\">\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>\n",
    "        <center>(a): Cartpole</center>\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>(b): Mountain Car</center>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "<center>Figure 24: Average Score vs Episode (Q-learning vs DQN)</center>\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td>\n",
    "        <img src=\"Cartpole Results/outputs/comparison of QL and DQL Average Reward.png\">\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"Mountain Car Results/outputs/comparison of QL and DQL Average Reward.png\">\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>\n",
    "        <center>(a): Cartpole</center>\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>(b): Mountain Car</center>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "<center>Figure 25: Average Reward vs Episode (Q-learning vs DQN)</center> \n",
    "\n",
    "The results from the testing trials confirm the inferences drawn from the training results showing that the Deep Q learning agent clearly outperforms the Q learning agent. \n",
    "For Cartpole, The Q learning agent manages to achieve a maximum value of 72 and 62 for score and total reward respectively, attaining a final value of 23.58 and 13.58 for average score and average reward. These values however dropped as low as 9, -1, 9 and -0.42 for the four metrics respectively. Hence it is clear that the Q learning agent was incapable for taking good decisions probably due to the occurrence of unprecedented states.\n",
    "However, the Deep Q learning agent manages to attain 499 and 489 as maximum values of score and total reward respectively. The values did drop to 238, 228, 430.67 and 420.67 respectively for the four evaluation metrics but the averages reached a final value of 468.5 and 458.5 showing the appreciable performance of the agent.\n",
    "Similarly, for Mountain Car, the Q learning agent The Q learning agent manages to achieve a maximum value of -0.17 for score with the average score attaining a final value of 23.58 but the total reward and average reward remain constant at -209 showing that the agent never manages to reach the goal state, showing that the Q learning agent was incapable for taking good decisions probably due to the occurrence of unprecedented states.\n",
    "However, the Deep Q learning agent manages to attain a constant score and average score of 0.54 showing that it managed to reach the goal state in all the testing trials performed attaining a maximum total reward of -118 and a final average reward of -151.46 showing the appreciable performance of the agent.\n",
    "\n",
    "## Random Exploration vs Q-Learning Agent vs DQN-Agent.\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td>\n",
    "        <img src=\"Cartpole Results/outputs/comparison of scores.png\">\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"Mountain Car Results/outputs/comparison of scores.png\">\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>\n",
    "        <center>(a): Cartpole</center>\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>(b): Mountain Car</center>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "<center>Figure 26: Score vs Episode (R.E vs Q-learning vs DQN)</center>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td>\n",
    "        <img src=\"Cartpole Results/outputs/comparison of total rewards.png\">\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"Mountain Car Results/outputs/comparison of total rewards.png\">\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>\n",
    "        <center>(a): Cartpole</center>\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>(b): Mountain Car</center>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "<center>Figure 27: Reward vs Episode (R.E vs Q-learning vs DQN)</center>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td>\n",
    "        <img src=\"Cartpole Results/outputs/comparison of average scores.png\">\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"Mountain Car Results/outputs/comparison of average scores.png\">\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>\n",
    "        <center>(a): Cartpole</center>\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>(b): Mountain Car</center>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "<center>Figure 28: Average Score vs Episode (R.E vs Q-learning vs DQN)</center>\n",
    "\n",
    " \n",
    "<table>\n",
    "<tr>\n",
    "    <td>\n",
    "        <img src=\"Cartpole Results/outputs/comparison of average rewards.png\">\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"Mountain Car Results/outputs/comparison of average rewards.png\">\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>\n",
    "        <center>(a): Cartpole</center>\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>(b): Mountain Car</center>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "<center>Figure 29: Average Reward vs Episode (R.E vs Q-learning vs DQN)</center>\n",
    "\n",
    "The comparison of the Q learning agent and the Deep Q learning agent with Random Exploration confirmed that the Q learning agent performs slightly better than Random Exploration since it achieves greater max and final values for the considered evaluation metrics hence adhering to the fact that simple Q learning is not completely ineffective but only in-sufficient to handle such environments of very huge state space. The DQN agent however showed remarkable performance in both the environments and clearly outclassed the Q learning agent even when trained for much lesser number of trials than the Q learning agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table>\n",
       "<center> Testing Videos</center>\n",
       "<tr>\n",
       "    <td>\n",
       "        <div align=\"middle\">\n",
       "        <video width = 90% height = 90% controls>\n",
       "            <source type =\"video/mp4\" src=\"Cartpole Results/Snaps/Cartpole Q_learning_agent testing.mp4\"/>\n",
       "        </video>\n",
       "        </div>\n",
       "    </td>\n",
       "    <td>\n",
       "        <div align=\"middle\">\n",
       "        <video width = 90% height = 90% controls>\n",
       "            <source type =\"video/mp4\" src=\"Cartpole Results/Snaps/Cartpole DQN_model_1 testing.mp4\"/>\n",
       "        </video>\n",
       "        </div>\n",
       "    </td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>\n",
       "        <center>Cartpole Q learning agent</center>\n",
       "    </td>\n",
       "    <td>\n",
       "        <center>Cartpole DQN</center>\n",
       "    </td>\n",
       "</tr>\n",
       "<tr>\n",
       "    <td>\n",
       "        <div align=\"middle\">\n",
       "        <video width = 90% height = 90% controls>\n",
       "            <source type =\"video/mp4\" src=\"Mountain Car Results/snaps/MountainCar Q_learning_agent testing.mp4\"/>\n",
       "        </video>\n",
       "        </div>\n",
       "    </td>\n",
       "    <td>\n",
       "        <div align=\"middle\">\n",
       "        <video width = 90% height = 90% controls>\n",
       "            <source type =\"video/mp4\" src=\"Mountain Car Results/snaps/MountainCar DQN_model_2 testing.mp4\"/>\n",
       "        </video>\n",
       "        </div>\n",
       "    </td>\n",
       "</tr>\n",
       "<tr>\n",
       "    <td>\n",
       "        <center>Mountain Car Q Learning agent</center>\n",
       "    </td>\n",
       "    \n",
       "    <td>\n",
       "        <center>Mountain Car DQN</center>\n",
       "    </td>\n",
       "</tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<table>\n",
    "<center> Testing Videos</center>\n",
    "<tr>\n",
    "    <td>\n",
    "        <div align=\"middle\">\n",
    "        <video width = 90% height = 90% controls>\n",
    "            <source type =\"video/mp4\" src=\"Cartpole Results/Snaps/Cartpole Q_learning_agent testing.mp4\"/>\n",
    "        </video>\n",
    "        </div>\n",
    "    </td>\n",
    "    <td>\n",
    "        <div align=\"middle\">\n",
    "        <video width = 90% height = 90% controls>\n",
    "            <source type =\"video/mp4\" src=\"Cartpole Results/Snaps/Cartpole DQN_model_1 testing.mp4\"/>\n",
    "        </video>\n",
    "        </div>\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "     <td>\n",
    "        <center>Cartpole Q learning agent</center>\n",
    "    </td>\n",
    "    <td>\n",
    "        <center>Cartpole DQN</center>\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>\n",
    "        <div align=\"middle\">\n",
    "        <video width = 90% height = 90% controls>\n",
    "            <source type =\"video/mp4\" src=\"Mountain Car Results/snaps/MountainCar Q_learning_agent testing.mp4\"/>\n",
    "        </video>\n",
    "        </div>\n",
    "    </td>\n",
    "    <td>\n",
    "        <div align=\"middle\">\n",
    "        <video width = 90% height = 90% controls>\n",
    "            <source type =\"video/mp4\" src=\"Mountain Car Results/snaps/MountainCar DQN_model_2 testing.mp4\"/>\n",
    "        </video>\n",
    "        </div>\n",
    "    </td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>\n",
    "        <center>Mountain Car Q Learning agent</center>\n",
    "    </td>\n",
    "    \n",
    "    <td>\n",
    "        <center>Mountain Car DQN</center>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The Results from the above simulation and study clearly show that Q-Learning shows remarkable performance in environments with a small State-Space, however fails to achieve the same in environments of higher dimensions. <br>\n",
    "<br>\n",
    "\n",
    "The Q-Learning agent does perform better than Random-Exploration but fails to achieve higher rewards or reach the goal state in many cases. The reason behind being the â€˜Curse of Dimensionalityâ€™, due to which the size of the Q-table increases exponentially and hence it becomes impossible to iterate over all the possible policies even with a high number of training trials resulting in the poor performance in such high dimensional spaces.<br>\n",
    "<br>\n",
    "\n",
    "Deep-Reinforcement Learning (here, Deep-Q Learning) manages to overcome this problem by following generalization.  The aim of the model is to figure the latent trends in the data gathered by exploring the environment and hence generate a model that can predict the possibly correct (not necessarily optimal) action. <br>\n",
    "<br>\n",
    "\n",
    "The results substantiate the same since the model manages to achieve very high scores and, in many instances, â€˜a perfect scoreâ€™ even in much lesser training trials. This strengthens the fact that the DQN-Agent overcomes the limitation of Policy Iteration Q-learning, to explore all states individually, and even with a much-limited exploration can offer remarkable results and hence outperforms the Policy Iteration Q-Learning algorithm.<br>\n",
    "<br>\n",
    "Also, the agents gives remarkable results for two different environments of same category without any alternation of model architechture or hyper-parameters showing that the agent is robust to environments of the same class for a long period of training and testing, hence prooving its reliability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scope for Improvement\n",
    "The results can be further strengthened by extending the Deep Q Learning algorithm over The Atari 2600 games as done in [9]. However, it required advance hardware requirements and hence was not included in this report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] M. Jaderberg, V. Mnih, W.M. Czarnecki, T. Schaul, J.Z. Leibo, D. Silver, K. Kavukcuoglu, \"Reinforcement learning with unsupervised auxiliary tasks\", Proc. Int. Conf. Learning Representations, 2017.\n",
    "\n",
    "[2] K. Arulkumaran, N. Dilokthanakul, M. Shanahan, and A. A. Bharath, â€œClassifying options for deep reinforcement learning,â€ in Proc. IJCAI Workshop Deep Reinforcement Learning: Frontiers and Challenges, 2016. \n",
    "\n",
    "[3] S. Gu, T. Lillicrap, I. Sutskever, and S. Levine, â€œContinuous deep Q-learning with model-based acceleration,â€ in Proc. Int. Conf. Learning Representations, 2016\n",
    "\n",
    "[4] A. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. Fearon, A. de Maria, V. Panneershelvam, M. Suleyman et al., \"Massively parallel methods for deep reinforcement learning\", ICML Workshop on Deep Learning, 2015.\n",
    "\n",
    "[5] N. Heess, J.J. Hunt, T.P. Lillicrap, D. Silver, \"Memory-based control with recurrent neural networks\", NIPS Workshop on Deep Reinforcement Learning, 2015.\n",
    "\n",
    "[6] Varsha Lalwani and MasareAkshaySuni. â€œPlaying Atari Games with Deep Reinforcement Learning.â€ , Technical Report, IIT Kanpur, 2015\n",
    "\n",
    "[7] KristjanKorjus, Ilya Kuzovkin, ArdiTampuu, Taivo Pungas. Replicating the Paper â€œPlaying Atari with Deep Reinforcement Learningâ€, Technical Report, Introduction to Computational Neuroscience, University of Tartu, 2013 \n",
    "\n",
    "[8] M.G. Bellemare, Y. Naddaf, J Veness and M. Bowling.â€The Arcade Learning Environment: An Evaluation Platform for General Agents.â€, Journal of Artificial Intelligence Research 47, Pages 253-279, 2013\n",
    "\n",
    "[9] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, M. Riedmiller, â€œPlaying Atari with Deep Reinforcement Learningâ€,  NIPS Deep Learning Workshop, 2013 \n",
    "\n",
    "[10] R. S. Sutton and A. G. Barto, â€œReinforcement Learning: An Introduction.â€, Cambridge, MA: MIT Press, 1998.\n",
    "\n",
    "[11] Christopher JCH Watkins and Peter Dayan. â€œQ-learning Machine learning,â€ 8(3-4):279â€“292, 1992.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
